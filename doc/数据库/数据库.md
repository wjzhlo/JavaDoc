# 数据库

## 数据库基础

### SQL 和 NoSQL

#### 数据库分类 

`简单地说就是关系型数据库使用SQL作为查询语言，数据存储格式必须严谨地以表形式存储。而菲关系型数据库即有自己的查询语言，且不使用表格存储数据，如Redis使用Key-Value形式存储，Mongo使用BSON形式存储 `

- 关系型数据库
  - MySQL、Oracle、DB2
  - 关系型数据库中都是表
- 非关系型数据库
  - MongoDB、Redis
  - 键值对数据库如 Redis
  - 文档数据库如 MongoDB



#### 两种数据库的对比

- **查询速度**
  - nosql 将数据都存储在缓存中，而关系型数据则是将数据存储在硬盘中，nosql 查询速度无疑更快
- **存储格式**
  - nosql 支持键值对形式、文档形式存储数据，而关系型数据库只能以表的形式存储数据。nosql 适用于业务的数据格式不固定或变动比较大的
- **复杂查询**
  - nosql 难以处理有关联性的复杂查询，而关系型数据库可以查询多个表完成复杂查询
- **事务支持**
  - nosql 不支持事务或支持很弱，而关系型数据库支持事务
- **一致性**
  - nosql 强调的是数据的最终一致性，即可能获取到的是中间态的数据。而关系型数据库则强调数据的强一致性



#### 各自的适用场景

NoSQL 适用于高读写，且对事务要求不高、查询不太复杂的业务类型，而关系型数据库则是提供了较好的事务支持以及表间的数据关联查询



#### NoSQL 四大分类

- KV键值对存储
  - Redis、Memecache、Tair
- 文档型数据库
  - Mongodb
- 列存储数据库
  - HBase、分布式文件系统
- 图形关系数据库
  - 如存储拓扑图的内容，存储的是关系而不是图片。一般用于朋友圈社交网络，或是广告推荐

<img src="数据库.assets/image-20210224162951857.png" alt="image-20210224162951857" style="zoom:80%;" />







## Mongodb

### 概念简介

#### 什么是 mongodb

`mongodb 是以文档形式存储数据的非关系型数据库，内部使用 BSON 这种无类型的数据格式来存储数据，支持快速集群和数据分片，具备灵活性、高效性、易扩展的特点`



#### BSON

`BSON(Binary JSON) 是一种类 json 的一种二进制形式的存储格式，具有轻量性、可遍历性、高效性等特点，灵活性高，但缺点是空间利用率低`



#### 优势

- 以 BSON 格式存储，无类型模式，易于快速开发，适用于需求快速迭代，数据结构变化频繁的业务
- 任何属性都支持建立索引
- 使用副本集能够快速搭建高可用性的mongodb集群环境，同时 FastOver 机制能够保证业务快速从故障中恢复
- 支持分片功能，能够快速水平扩展，存储更多的数据
- 更丰富的查询支持，如对地理位置的查询



#### 适用场景

`mongodb 适用于日志系统、消息存储系统这一类存储格式可能不统一或容易变动的业务，同时也适用于地理信息查询的业务存储。具有高可用性、易扩展性、高读写能力以及较低的学习成本。在不强调事务以及复杂的表联合的情况，可以考虑使用 mongodb 来作为数据库`

- 日志系统、消息存储系统
  - 由于 Mongodb 的 schema-less(无模式)，即一个集合中可以放任意格式的数据，使得开发者无需为格式不同的信息专门设计统一的格式，减少开发成本，且增加字段无需修改表结构，学习成本低
- 地理信息的查询业务(打车、外卖、定位等)
  - Mongodb 为地理坐标提供了专门的存储类型，开发者可以将地理位置存储在 Mongodb 中，然后通过 Mongodb 进行地理位置查询，Mongodb 也支持创建地理位置索引



**以下为考虑条件：**

- 应用不要求事务以及复杂表联合查询(必须满足)
- 数据模型无法完全确定，会随着需求变化，需要快速迭代开发
- 需要保证高读写性能
- 应用需要保持大数据量存储和快速水平扩展的能力(分片)
- 需要高可用，业务数据不丢失(副本集、FastOver)
- 需要大量进行地理位置查询、文本查询



#### 数据类型

- double
- int
- long
- bool
- object
- string
- array
- date
- 正则
- javascript
- ...



#### BSON 与 JSON 的对比

JSON 是像字符串一样存储，而 BSON 是按结构存储的

- 遍历更快
  - 在 JSON 中如果要跳过一个文档进行数据读取，需要扫描过这个文档才可以，且要对它进行数据规则匹配(如括号匹配)
  - 而 BSON 是按结构存储的，它在每个元素的头部都存储了这个元素的长度，要跳过一个文档只需要读取到元素长度，直接偏移即可
- 结构修改更快
  - JSON 是像字符串一样存储数据的，当数据长度发生变化(如a变成aa)，则需要将后面的所有数据都向后移动一位。
  - 而 BSON 是按结构存储的，支持不同的数据类型，合理的数据变化不会导致影响到其他数据



#### 基本概念

- 数据库(database)
  - 数据库是一个仓库，在仓库中可以存放一个集合
- 集合(collection)
  - 集合是类似数组(表)，在集合中可以存放文档。集合中的文档不要求格式相同
- 文档(document)
  - 文档是数据库中的最小的安慰，等同于关系型数据库的表中的一条记录

![image-20210221205405595](数据库.assets/image-20210221205405595.png)



#### 默认库

- admin
  - 等用于 root 数据库，如果将用户添加到这个库中，则这个用户拥有所有库的操作权限
- local
  - 用于存储本地单台服务器的任意集合
- config
  - 当 mongo 用于分片设置时，config 数据库用于保存分片的相关信息



### Mongodb、Mysql、Redis 三者对比

- **Mongodb 和 Mysql 的对比**
  
  - **灵活性：**BSON 是模型自由的，一个集合中可以存储各种类型的文档。而 mysql 则需要严谨地维护各个表
  
  - **读写效率：**Mongodb 侧重于大量读写操作，而非事务安全。效率上 mongodb 与 mysql 差不多
  
  - **可用性：**Mongodb 的副本集形式，FastOver 能够让 mongo 服务快速从故障中恢复使用。而 mysql 则需要手动切换
  
  - **搭建难度：**Mongodb 的副本集搭建简单，且支持数据分片
  
  - **事务：**Mongodb 不支持事务，mysql 支持事务
  
  - **适用场景：**Mongodb 更适用于高并发读写，且对数据关联要求不高，数据业务相对简单的系统，如日志系统或聊天消息服务器。
  
    ​				    而 Mysql 适用于处理关联性强，数据一致性要求高的系统



- **Mongodb 和 Redis 的对比**

  - **内存存储：**Mongodb 将数据存在内存中，但当内存不够时会将数据存储在磁盘中，内存只存储热点数据

    ​				  Redis 将数据存储在内存中，定期写入磁盘。但当内存不够时，需要使用内存淘汰策略清理掉不常用的数据

  - **支持的数据结构：**Redis 支持设置 hash、set、list 等多种结构，但 Mongodb 的集合和文档形式，更贴近于数据库的使用方式，包含了丰

    ​							 富的数据格式以及索引设置

  - **性能：**Redis > mongodb

  - **可用性：**Mongodb 支持快速集群以及分片，且提供快速水平扩展的能力。而 Redis 的集群示例增减或数据重新分布后，会导致哈希槽和

    ​			  实例的映射变化导致客户端请求出错

  - **查询语言：**Mongodb 提供了更丰富的查询语言



### 指令

https://docs.mongodb.com/manual/crud/

#### 数据库操作

mongodb 中数据库和集合都会自动创建。不用管他是否存在

```
# 显示所有数据库
show dbs;

# 进入到指定数据库
use [数据库名]

# 查看当前在哪个数据库中
db

# 删除数据库
先进入数据库
use user
删除数据库
user.dropDatabase()
```



#### 集合操作

```
# 显示数据库中的所有集合
show collections

# 删除集合
db.users.drop()
```



##### 插入操作

```
# 插入一条数据
db.users.insert({name:"tom", age:18})

# 插入多个
db.users.insert([{name:"tom1", age:18},{name:"tom2", age:18},{name:"tom3", age:18}])
```



##### 查询操作

###### 条件查询

```
############ 条件查找 ############
## 查找集合中的所有数据
db.users.find();

## 根据属性查找全部，返回的是一个数组
db.users.find({age: 18})
db.users.find({age: 18, name: "tom1"})

## 查找符合条件的第一个，返回的是文档对象
db.users.findOne({age: 18})

## 查询符合条件的结果个数
db.users.find().count()

## 指定返回的内容(投影)
# 仅显示员工姓名 显示1 不显示0
db.emp.find({}, {ename:1, _id: 0})

# 格式化查找结果
db.collectionName.find().pretty()
```



###### 比较查询

```
## 查找age大于18 $gte
# $lt 小于
  $gt 大于
  $gte 大于等于
  $lte 小于等于
  $eq 等于
  $ne 不等于
db.users.findOne({age: {$gte: NumberInt(18)}})
```



###### 分页查询

```
## 获取指定范围
# 查找前10条满足条件的
db.users.find().limit(10)

# 查找 11~20 条满足条件的
db.users.find().skip(10).limit(10)
```



###### 排序查询

```
# 按工资进行升序排列  1升序 -1降序
db.emp.find().sort({sale:1}) 

# 先按照工资排序，若工资相同，按照部门编号降序排列
db.emp.find().sort({sale:1, empno:-1}) 

```



###### 正则查询

```
# 查询员工姓名中含有 to 的
db.emp.find({ename: /to/})
# 查询员工姓名以 bob 开头的
db.emp.find({ename: /^bob/})
```



###### 包含 $in、不包含 $nin

```
# 查询员工工资是1000和2000的
db.emp.find({sale:{$in:[1000,2000]}})

# 查询员工工资不是1000和2000的
db.emp.find({sale:{$nin:[1000,2000]})
```



###### 条件查询 $and  $or

```
# 查询年龄大于等于20，且小于35的文档
db.users.find({$and:[{age:{$gte:NumberInt(20)}},{age:{$lte:NumberInt(35)}]})

# 查询年龄小于20或是年龄大于等于30的文档
db.users.find({
    $or: 
    [{
            age: {
                $lt: NumberInt(20)
            }
        },
        {
            age: {
                $gte: NumberInt(30)
            }
        }
    ]
})
```







##### 修改操作

```
## 修改数据或添加属性
# 前面是匹配条件，后面是修改内容
# 必须加 $set，否则会直接用新对象覆盖旧对象
# update 默认只会修改一个
db.users.update({name:"tom2"}, {$set:{age: 11}});

## 删除属性
# 删除属性使用 $unset，只要提供key即可，无所谓value是多少
db.users.update({name:"tom2"}, {$unset:{age: 1}});

## 更新一个
db.users.updateOne({name:"tom2"}, {$set:{age: 11}});

## 更新多个
db.users.updateMany({name:"tom2"}, {$set:{age: 1}});
```



##### 删除操作

```
## 删除，直接用 remove 是删除所有符合条件
# 删除集合中的全部文档，但性能很差
db.users.remove();
# 删除所有符合条件的文档
db.users.remove({age:28})

## 删除满足条件的第一个
db.users.deleteOne({age:28})
```



#### 聚合操作

`聚合操作即对数据进行复杂的处理，如分组，条件判断，计算总和、设置别名等`

| SQL 操作/函数 | mongodb聚合操作     |
| ------------- | ------------------- |
| where         | $match              |
| group by      | $group              |
| having        | $match              |
| select        | $project            |
| order by      | $sort               |
| limit         | $limit              |
| sum()         | $sum                |
| count()       | $sum                |
| join          | $lookup （v3.2 新增 |

```sql
db.orders.aggregate( [
   {
     $group: {
        _id: null,
        total: { $sum: "$price" }
     }
   }
] )

# 类似mysql
SELECT SUM(price) AS total  FROM orders
```



#### 其他

查看当前占用的线程数量

```
db.serverStatus().connections;
```



### mongo 索引

https://docs.mongodb.com/manual/indexes/#single-field

#### 索引的作用

`索引是用来帮助数据库高效获取数据的数据结构，在 mongo 中索引的数据结构是B树，而 mysql 中则是B+树`

索引有序存储了特定的字段或一组字段，支持根据索引快速匹配或获取到对应的数据。如果没有索引，则数据库需要遍历扫描表或是集合中的全部数据，在处理大数据量时这种查找的效率无疑是很低的



#### 索引的优缺点

**优点：**

- 提高了检索效率，减少全表扫描
- 在分组和排序类型的搜索时，也可以减少分组或排序的时间

**缺点：**

- 占用内存空间
- 仅仅提高了读的效率，降低了增删改的效率，对表数据进行增删改操作时，索引也要动态维护。且随着索引数据越来越大，耗费的时间也逐步增加



#### 索引类型

##### 单字段索引

`mongo 支持给文档中的单字段进行升序或降序排列`



##### 多字段索引

`mongo 支持给文档中的多字段进行升序、降序排列，字段排列是逐步过滤的过程，下一个索引的排序内容是根据上一个索引排序后的结果基础上继续处理的`



##### 地址空间索引

`支持对地理空间坐标的有效查询，提供了两种特殊索引：返回平面几何的二维索引和返回球面集合的二维索引`



##### 文本索引

`对文本进行分词查询`



##### 哈希索引

`分片时使用`



#### 索引使用

查看索引

```
db.users.getIndexes();
```

创建索引

```
# 单索引
db.users.createIndex({age:1})

# 复合索引
db.users.createIndex({age:1, name:-1})
```

移除索引

```
# 以索引名删除，index 为索引名称
db.collection.dropIndex(index)

# 条件删除，删除所有age排序为升序的索引
db.collection.dropIndex({age:1})

# 删除全部
db.collection.dropIndex()
```



#### 执行计划(运行分析)

##### 概念

`执行计划即性能分析，如查看查询耗费时间，是否基于索引查询等`

##### 用法

```
db.users.find({userid:1}).explain()
```

<img src="数据库.assets/image-20210222142601845.png" alt="image-20210222142601845" style="zoom: 80%;" />







### 环境搭建

#### Windows 环境搭建

**下载安装包**

```
https://fastdl.mongodb.org/windows/mongodb-windows-x86_64-4.4.4-signed.msi


# 在 cmd 中启动客户端
mongo
```

**配置环境变量**

找到安装目录的 bin 文件夹，放入系统变量 path 中

**创建默认的数据存储文件夹**

在C盘下创建data文件夹，在data文件夹下创建db文件夹和log文件夹

**开启数据库**

在 cmd 中启动

```
#  默认启动 c:\data\db 端口为 27017
mongod 

# 指定 db 路径，指定端口号启动
mongod --dbpath [xxxxdb路径] --path [端口号]
```

开启客户端

```
mongo
```



#### CentOS 下搭建

https://www.cnblogs.com/jasonLiu2018/p/13088971.html



##### 启动脚本

```
./bin/mongod --config /usr/local/mongodb/mongodb.conf
```



##### 关闭脚本

```
./bin/mongod  --shutdown --config /usr/local/mongodb/mongodb.conf
```



### 副本集(集群)

#### 概念和特点

`mongo 副本集是一组维护相同集合的mongo服务，提供了冗余和高可用性，包括自动故障恢复、主从切换、读写分离等功能`



#### 角色和选举

- Primary：主要成员
  - 主节点，主要接受读数据
- Replicate：从节点
  - 从主节点通过复制来维护相同的数据集合，即数据备份。但不可以进行写操作。可以进行读操作(需要设置 secondaryOk())
- Arbiter：仲裁者
  - 不保留副本数据，只具有投票选举的作用，它能够通过响应其他副本集成员的心跳和选举请求来维护副本集中的仲裁
  - **是否添加仲裁者：**
    - 若副本+主节点的个数是偶数，则需要添加一个仲裁者形成奇数，满足“大多数投票”这个要求
    - 若副本+主节点的个数已经是奇数，则可以不用添加仲裁者

<img src="数据库.assets/image-20210223092206892.png" alt="image-20210223092206892" style="zoom:67%;" />



#### 主节点的选举规则

###### 选举的触发条件

mongodb 副本集会自动选择主节点，发生主节点选举的触发条件有：

- 主节点故障
- 主节点网络不可达(通过心跳，10秒)
- 人工干预



###### 选举规则

- 票数最高，且获得了“大多数”成员的投票支持的节点获胜
  - **“大多数”：**若副本集内的节点数量为N，则大多数=(N/2)+1。如副本集中有三个节点，则大多数=2
- 若票数相同，且都获得了“大多数”成员的支持，则数据新的节点获胜
  - **数据的新旧：**通过操作日志 oplog 来对比的
- 当副本集中当前存活节点数量不满足大多数，即死掉了一半的节点，则整个副本集都不存在 Primary 节点，整个副本集都处于可读不可写的状态



###### **修改优先级**

修改优先级必须在 Master 节点的客户端中

```
var cfg=rs.conf();
cfg.members[0].priority=1
cfg.members[1].priority=1
cfg.members[2].priority=10
rs.reconfig(cfg);
```

在修改完成后，一段时间后就会开始重新选举



#### 一主一副本一仲裁搭建

##### 下载mongodb

下载副本

```
https://www.mongodb.com/try/download/community

https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.4.tgz
```

需要写好服务器类型以及下载的包类型

<img src="数据库.assets/image-20210223092455619.png" alt="image-20210223092455619" style="zoom:67%;" />



##### 配置

###### 解压和移动项目

```markdown
# 解压
tar -zxvf mongodb-linux-x86_64-rhel70-4.4.4.tgz
mv mongodb-linux-x86_64-rhel70-4.4.4 mongodb27017

# 创建集群文件夹
cd /usr/local
mkdir /usr/local/mongo-cluster
mv mongodb27017 mongo-cluster

# 创建对应的日志文件目录和数据存储目录
mkdir /usr/local/mongo-cluster/mongodb27017/data
mkdir /usr/local/mongo-cluster/mongodb27017/data/db
mkdir /usr/local/mongo-cluster/mongodb27017/data/log
touch /usr/local/mongo-cluster/mongodb27017/data/log/mongo.log
touch /usr/local/mongo-cluster/mongodb27017/data/log/mongo.pid
```



###### 增加配置文件

```markdown
# 进入主节点的目录
cd mongodb27017

# 新增配置文件
vim mongodb.conf
```

配置文件内容为：

```markdown
# net
port=27017 #端口
bind_ip=0.0.0.0 #副本集模式下只能这样设置才能被外网访问，否则会报错

# systemLog
#destination=file
logpath= /usr/local/mongodb-cluster/mongodb27017/data/log/mongod.log #日志文件存放路径
logappend=true #使用追加的方式写日志

# storage
dbpath= /usr/local/mongodb-cluster/mongodb27017/data/db #数据库存文件存放目录
journal=true #每次写入会记录一条操作日志（通过journal可以重新构造出写入的数据。启用或禁用持久性日志以确保数据文件保持有效和可恢复。）。

# processManagement
fork=true #以守护进程的方式运行，创建服务器进程
# maxConns=100 #最大同时连接数
pidfilepath=/usr/local/mongodb-cluster/mongodb27017/data/log/mongod.pid

# connect
noauth=true #不启用验证

#即使宕机，启动时wiredtiger会先将数据恢复到最近一次的checkpoint点，然后重放后续的journal日志来恢复。
storageEngine=wiredTiger  #存储引擎有mmapv1、wiretiger、mongorocks

# 定义副本集名称
replSet=mongo-replication
```



###### 增加启动脚本和关闭脚本

启动脚本

```
vim start.sh
```

```
/usr/local/mongodb-cluster/mongodb27017/bin/mongod -f /usr/local/mongodb-cluster/mongodb27017/mongodb.conf
```

关闭脚本

```
vim stop.sh
```

```
/usr/local/mongodb-cluster/mongodb27017/bin/mongod  --shutdown --config /usr/local/mongodb-cluster/mongodb27017/mongodb.conf
```

客户端访问脚本

```
vim client.sh
```

```
./bin/mongo --host 172.16.204.71 --port 27017
```

授予执行权限

```
chmod +711 start.sh stop.sh client.sh
```



###### 从节点和仲裁节点搭建

**复制文件**

```markdown
# 复制出三份，分别作为主节点、从节点以及仲裁者
cd /usr/local/mongodb-cluster/mongo-cluster
# 原本的27017作为主节点，复制出一个从节点 27018
cp -a mongodb27017 mongodb27018
# 复制出一个27019作为仲裁节点
cp -a mongodb27017 mongodb27019
```



**节点搭建**

实际就是将 配置文件、启动脚本、关闭脚本、客户端脚本中的端口号27017全部改为27018

仲裁节点则是改成 27019

```markdown
# 进入从节点文件夹
cd mongodb27018

# 修改配置文件，其实就是修改端口号 27017 为 27018
vim mongodb.conf
```

```markdown
# net
port=27018 #端口
bind_ip=0.0.0.0 #副本集模式下只能这样设置才能被外网访问，否则会报错

# systemLog
#destination=file
logpath= /usr/local/mongodb-cluster/mongodb27018/data/log/mongod.log #日志文件存放路径
logappend=true #使用追加的方式写日志

# storage
dbpath= /usr/local/mongodb-cluster/mongodb27018/data/db #数据库存文件存放目录
journal=true #每次写入会记录一条操作日志（通过journal可以重新构造出写入的数据。启用或禁用持久性日志以确保数据文件保持有效和可恢复。）。

# processManagement
fork=true #以守护进程的方式运行，创建服务器进程
# maxConns=100 #最大同时连接数
pidfilepath=/usr/local/mongodb-cluster/mongodb27018/data/log/mongod.pid

# connect
noauth=true #不启用验证

#即使宕机，启动时wiredtiger会先将数据恢复到最近一次的checkpoint点，然后重放后续的journal日志来恢复。
storageEngine=wiredTiger  #存储引擎有mmapv1、wiretiger、mongorocks

# 定义副本集名称
replSet=mongo-cluster
```



修改启动脚本

```
/usr/local/mongodb-cluster/mongodb27018/bin/mongod -f /usr/local/mongodb-cluster/mongodb27018/mongodb.conf
```

修改关闭脚本

```
/usr/local/mongodb-cluster/mongodb27018/bin/mongod  --shutdown --config /usr/local/mongodb-cluster/mongodb27018/mongodb.conf
```

修改客户端访问脚本

```
./bin/mongo --host 172.16.204.71 --port 27018
```



**仲裁节点搭建**

同上，但是端口号改为27019 。。略...



###### 创建集群启动脚本和关闭脚本

**集群启动脚本**

```
cd /usr/local/mongodb_cluster
vim cstart.sh
```

```
sh ./mongodb27017/start.sh
sh ./mongodb27018/start.sh
sh ./mongodb27019/start.sh

echo "服务启动完成..."

ps -ef|grep mongod
```

**集群关闭脚本**

```
vim cstop.sh
```

```markdown
sh ./mongodb27017/stop.sh
sh ./mongodb27018/stop.sh
sh ./mongodb27019/stop.sh
echo "服务关闭完成..."
ps -ef|grep mongod
```

**授予执行权限**

```
chmod +711 cstart.sh cstop.sh
```



##### 启动集群

```
sh cstart.sh
```

![image-20210223095333642](数据库.assets/image-20210223095333642.png)



##### 集群设置

###### 进入主节点客户端

```markdown
# 先进入27017查看
cd /usr/local/mongodb-cluster/mongodb27017

# 使用客户端进入
sh client.sh
```

若是主节点，则会显示 PRIMARY。若是从节点，则显示 SECONDARY

![image-20210223095537341](数据库.assets/image-20210223095537341.png)



###### 配置副本集(！需要进入主节点客户端)

**初始化副本集**

```markdown
rs.initiate()
```

<img src="数据库.assets/image-20210223095854225.png" alt="image-20210223095854225" style="zoom:67%;" />

**添加从节点**

```markdown
# 在主节点中先配置从节点
rsconf = {
    _id: "mongodb-cluster",
    members: [
         {
          _id: 0,
          host: "121.36.203.234:28001",
            "priority":2
         },
         {
          _id: 1,
          host: "123.60.11.63:28002",
            "priority":1
         }
    ]
}
 
# add 内填入公网地址
rs.add("www.xxx.top:27018")
```

**添加仲裁者**

```
rs.addArb("www.xxx.top:27019")
```



##### 常用命令

###### 查看配置信息

```
rs.conf()
```

<img src="数据库.assets/image-20210223100404644.png" alt="image-20210223100404644" style="zoom: 50%;" />



###### 查看副本集状态

```
rs.status();
```

<img src="数据库.assets/image-20210223101400026.png" alt="image-20210223101400026" style="zoom:67%;" />



##### 修改从节点可读

由于从节点默认是不可读的，要从从节点科幻读取数据必须进入客户端进行设置

```
# 先进入27017查看
cd /usr/local/mongodb-cluster/mongodb27017

# 使用客户端进入
sh client.sh
```

```
# 设置为从节点数据可读
rs.secondaryOk()

# 取消从节点数据可读
rs.secondaryOk(false)
```

```
# 此时可以查看数据了
show dbs;
```



##### 阿里云外网地址变成内网地址

使用代码连接时发现获取到的是内网地址，进入服务器客户端查看

![image-20210223101748645](数据库.assets/image-20210223101748645.png)



在客户端手动修改为外网地址

```
var config = rs.config();
config.members[0].host="www.gtom.top:27017"
config.members[1].host="www.gtom.top:27018"
config.members[2].host="www.gtom.top:27019"
rs.reconfig(config);
```



### 分片集群 Sharded Cluster

#### 分片

##### 概念

`分片就是将数据拆分，将其分散存储在不同的机器上的过程`



##### 角色

- **Shard(分片)：**每个分片都包含分片数据的子集，每个分片都可以部署为数据集
- **mongos(路由)：**mongos 是查询路由器，给客户端和分片集群之间提供接口。客户端需要访问路由来查询数据
- **config servers(配置服务)：**配置服务器存储集群的元数据和配置设置。mongodb3.4开始要求配置服务必须是副本集形式的

<img src="数据库.assets/image-20210223110850595.png" alt="image-20210223110850595" style="zoom:67%;" />



#### 搭建方式

见 mongodb_advance.pdf

链接：https://pan.baidu.com/s/1Nb51JbaliXN9rcbUfVdmsw 
提取码：z3h2 



### 安全认证

#### 使用

##### 创建管理员账号

需要先进入 primary 节点的客户端

```
# 切换到 admin 库下
use admin;

# 创建超级管理员账户
db.createUser({user:"myroot",pwd:"123456",roles:["root"]});

# 生成 key
openssl rand -base64 90 -out ./mongo.keyfile

# 设置 keyfile 只有当前用户可读
chmod 400 mongo.keyfile
```

修改配置文件 mongod.conf

```
# 指定私钥位置
keyFile=/usr/local/mongodb-cluster/mongodb27017/mongo.keyfile
# 启动安全策略
auth=true
```

将 keyfile 文件复制到其他副本集的节点上

```
# 将 keyfile 文件复制到其他副本集的节点上
cp mongo.keyfile ../mongodb27018
cp mongo.keyfile ../mongodb27019
```



##### 登录验证

之后使用客户端登录，登录后需要验证账号密码

```
use admin;
db.auth("myroot","123456");
```

或是在使用登录脚本时写上账号密码

```
./bin/mongo --host 172.16.204.71 --port 27018 --authenticationDatabase admin -u myroot -p 123456
```



##### 创建用户账号

切换到对应的库

```
# 切换到对应的库
use test;

# 给库添加普通用户
db.createUser({user:"tom",pwd:"123456",roles:["readWrite"]})
```



### SpringData 连接示例

https://blog.csdn.net/xjmfc/article/details/106929136



#### 示例

引入依赖

```xml
<properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <java.version>1.8</java.version>
    <spring.boot.version>2.3.7.RELEASE</spring.boot.version>
    <lombok.version>1.18.0</lombok.version>
</properties>

<dependencies>
    <!--  springboot相关 -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
        <version>${spring.boot.version}</version>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-dependencies</artifactId>
        <version>${spring.boot.version}</version>
        <type>pom</type>
        <scope>import</scope>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
        <version>${spring.boot.version}</version>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <version>${spring.boot.version}</version>
        <scope>test</scope>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-mongodb</artifactId>
        <version>${spring.boot.version}</version>
    </dependency>
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>${lombok.version}</version>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-devtools</artifactId>
        <scope>runtime</scope>
        <optional>true</optional>
    </dependency>
</dependencies>
```



**修改启动类**

```
spring:
  data:
    mongodb:
      # secondaryOk=true 开启副本节点的读功能，可以实现读写分离
      # connect=replicaSet 自动到副本集中选择读写数据，需要slaveOk是打开的
      # 若开启了验证，则需要在 mongodb:// 后加上 [账号]:[密码]，如 tom:123456
      uri: mongodb://tom:123456@www.gtom.top:27017,www.gtom.top:27018,www.gtom.top:27019/artical?connect=replicaSet&secondaryOk=true&replicaSet=mongo-cluster-27001
```



**增加集合类 Comment**

```java
@Data
// 默认使用全小写的类名作为集合
@Document(collection = "comment")
public class Comment implements Serializable {
    private static final long SerialVersionUID = 1L;

    @Id
    private String id; // 主键

    @Field("content")
    private String content; // 吐槽内容
    private Date publishTime; // 发布日期

    @Indexed
    private String userId; // 发布人ID

    private String nickname; // 昵称
    private LocalDateTime createDatetime; // 评论的日期时间
    private Integer likeNum; // 点赞数
    private Integer replyNum; // 回复数
    private String state; // 状态
    private String parentId; // 上级id
    private String articalId; // 对应的文章id
}
```



**增加对应的 Repository**

findByParentId是固定写法，不能随便改

```java
public interface CommentRepository extends MongoRepository<Comment, String> {
    Page<Comment> findByParentId(String parentId, Pageable pageable);
}
```



编写对应的 service

```java
@Service
public class CommentService {

    @Autowired
    private CommentRepository commentRepository;

    public void saveComment(Comment comment) {
        commentRepository.save(comment);
    }

    public void updateComment(Comment comment) {
        commentRepository.insert(comment);
    }

    public void deleteComment(String id) {
        commentRepository.deleteById(id);
    }

    /**
     * 获取所有评论
     * @return
     */
    public List<Comment> findCommentList() {
        return commentRepository.findAll();
    }

    public Comment findCommentById(String id) {
        return commentRepository.findById(id).get();
    }

    public Page<Comment> findCommentListByParentId(String parentId, int page, int size) {
        return commentRepository.findByParentId(parentId, PageRequest.of(page, size));
    }
}
```



**测试类**

```java
@RunWith(SpringRunner.class)
@SpringBootTest(classes = ArticalApplication.class)
class CommentServiceTest {

    @Autowired
    private CommentService commentService;

    @Test
    void saveComment() {
        Comment comment = new Comment();
        comment.setArticalId(UUID.randomUUID().toString());
        comment.setContent("测试的评论004");
        comment.setCreateDatetime(LocalDateTime.now());
        comment.setUserId("10086");
        comment.setNickname("超级凯撒");
        comment.setState("1");
        comment.setLikeNum(0);
        comment.setReplyNum(0);
        comment.setParentId("0");
        commentService.saveComment(comment);

        findCommentList();
    }

    @Test
    void updateComment() {
        Comment comment = commentService.findCommentById("60336d9b646b171912719a51");
        comment.setReplyNum(1);
        commentService.updateComment(comment);
        findCommentList();
    }

    @Test
    void deleteComment() {
        commentService.deleteComment("60336d9b646b171912719a51");
    }

    @Test
    void findCommentList() {
        List<Comment> commentList = commentService.findCommentList();
        commentList.forEach((Comment comment) -> {
            System.out.println(comment.toString());
        });
    }

    @Test
    void findCommentById() {
    }

    @Test
    void findCommentListByParentId() {
        Page<Comment> commentPage = commentService.findCommentListByParentId("0", 0, 10);
        System.out.println(commentPage.getTotalElements());
        System.out.println(commentPage.getContent());
    }
}
```



#### Save 和 Insert 的区别

- 主键存在时
  - Insert 会报异常 DuplicateKeyException 提示主键重复，且不保存当前数据
  - save 会对当前已经存在的数据进行修改操作
- 效率
  - Insert 不会进行遍历操作，且可以一次性批量插入
  - save 需要遍历列表，且一个个进行插入



### 备份

#### 备份/恢复工具下载

需要先从官网下载 Mongodb 的备份/恢复 工具

https://www.mongodb.com/try/download/database-tools



#### 全量备份脚本

```shell
#!/bin/bash 
host='www.xxx.top'
port='27017'
sourcepath='/usr/local/mongodb-cluster/mongodb-tools/bin'
targetpath='/usr/local/mongodb-cluster/backup'
nowtime=$(date "+%Y%m%d")
targetname=mongodb${nowtime}
username="myroot"
password="123456"
 
start(){
    $sourcepath/mongodump --host $host --port $port --authenticationDatabase admin -u ${username} -p ${password} --oplog --gzip --out ${targetpath}/${targetname}
}
 
execute(){
echo "=========================$(date) backup all mongodb back start  ${nowtime}========="
start
if [ $? -eq 0 ];then
    echo "The MongoDB BackUp Successfully!"
else
    echo "The MongoDB BackUp Failure"
fi
}
 
if [ ! -d "${targetpath}/${targetname}" ];then
    mkdir -p "${targetpath}/${targetname}"
fi
 
execute
 
backtime=$(date -d '-7 days' "+%Y%m%d")
if [ -d "${targetpath}/${backtime}/" ];then
    rm -rf "${targetpath}/${backtime}/"
    echo "=======${targetpath}/${backtime}/===删除完毕=="
fi
 
echo "========================= $(date) backup all mongodb back end ${nowtime}========="
```



#### 全量恢复脚本

```shell
#!/bin/bash
 
echo -e "\033[31;1m*****[ Mongodb ] 全库恢复脚本*****\033[0m"
host='www.gtom.top'
port='27017'
mongo_bin='/usr/local/mongodb-cluster/mongodb-tools/bin'
nowtime=$(date "+%Y%m%d")
username="myroot"
password="123456"
backpath='/usr/local/mongodb-cluster/backup' 
 
echo -e "\033[32;1m[ 选择要恢复全库的日期 ] \033[0m"
for backfile in `ls $backpath`; do
    echo $backfile
done
 
read -p ">>>" date_bak
 
if [[ $date_bak == "" ]] || [[ $date_bak == '.' ]] || [[ $date_bak == '..' ]]; then
    echo -e "\033[31;1m输入不能为特殊字符.\033[0m"
    exit 1
fi
 
 
if [ -d $backpath/$date_bak ];then
    read -p "请确认是否恢复全库备份[y/n]:" choice
 
    if [ "$choice" == "y" ];then
        echo -e "\033[32;1m正在恢复全库备份，请稍后...\033[0m"
        $mongo_bin/mongorestore --host $host --port 27017 --oplogReplay  --authenticationDatabase admin -u ${username} -p ${password} --gzip $backpath/$date_bak/
        if [ $? -eq 0 ];then
            echo -e "\033[32;1m--------全库恢复成功.--------\033[0m"
        else
            echo -e "\033[31;1m恢复失败,请手动检查!\033[0m"
            exit 3
        fi
    else
        exit 2
    fi
else
    echo "\033[31;1m输入信息错误.\033[0m"
    exit 1
fi
```



#### 增量备份/增量恢复

https://www.cnblogs.com/hukey/p/11512062.html

https://www.cnblogs.com/xuliuzai/p/9917137.html



### 碎片整理





### 待处理：

```
碎片整理

设置oplog大小，防止被覆盖
replSetResizeOplog

https://blog.csdn.net/u011305680/article/details/79730646

http://mongoing.com/archives/27228
```



## Redis

http://www.redis.cn/

### 基本概念

#### 概述

`Redis(Remote dictionary Server) 远程服务字典，是一个基于内存的可持久化的非关系型数据库。使用KV键值对的方式存储数据。支持持久化、主从复制以及集群`

但 Redis 带来数据读写效率提供的同时，也无法保证数据的强一致性



#### 使用场景

- 计数器、关注、点赞
  - 可以对 String 进行自增自减运算，从而实现计数器功能。由于 Redis 的读写性能非常高，很适合用于频繁读写。如用户的关注，点赞数，然后每隔一段时间在存储到数据库中
- 缓存
  - 将热点数据放到内存中，降低数据库的访问压力。需要设置内存的最大使用量以及淘汰策略以保证缓存的命中率。
- 全局会话缓存
  - 使用 Redis 存储用户的 Session，用户可以访问项目中的任意服务器，服务器可以从 Redis 读取用户的 Session 状态来判断是否允许访问。同时可以设置 Redis 中存储的 Session 过期时间，实现超时管理
- 消息队列
  - Redis 支持发布/订阅功能，哨兵机制就是基于发布/订阅功能实现的
- 分布式锁
  - Redis 支持使用 set 实现分布式锁，实现多线程访问同一个变量的线程安全
  - https://www.cnblogs.com/agilestyle/p/13200032.html
  - https://www.cnblogs.com/houchen/p/13801103.html
- 共同好友、好友推荐
  - 使用 Set 或是 Hyperloglog 的交集实现共同好友，或是好友推荐
- 排行版
  - ZSet 维护了一个有序的不重复集合，可以很方便地实现排行版功能



### 基本知识

默认有16个数据库[0~15]，默认使用第0个

```markdown
# 切换数据库
select [数据库号]
select 3

# 查看数据库数据大小
DBSIZE

# 清除 当前数据库
flushdb

# 清除全部数据库内容
flushall
```

基本命令

```markdown
# 查看所有key
keys *

# 判断key是否存在
EXISTS tmpList

# 删除key
move tmpList 1

# 设置/取出
get tmpList
set tmpList 100

# 给值设置过期时间，10秒过期
expire tmpList 10

# 查看值什么时候过期 -2即过期，值被删除
ttl tmpList

# 查看存储的数据类型
type tmpList
```



#### Redis 为什么是单线程的

因为 Redis 是基于内存操作的，数据的读写都在内存中，因而 CPU 不是 Redis 的瓶颈，内存和网络带宽才是 Redis 的瓶颈，就无需使用多线程来处理。同时，使用单线程还能避免线程上下文切换的开销，对于内存系统来说，没有上下文切换的消耗，就是效率最高的情况



#### 为什么 Redis 这么快

- 完全基于内存，请求处理效率十分快速
- 数据结构简单，对数据操作也简单
- 采用单线程，避免了不必要的上下文切换和竞争，不存在线程切换的消耗，也不用考虑锁的问题，不存在加锁释放锁的问题
- 使用多路复用IO



### 数据类型

#### 分类

- 五大基本数据类型
  - String、List、Set、Hash、Zset
- 特殊数据类型
  - geospatial 地理空间存储和处理
  - Hyperloglog 基数统计(不重复数据统计)
  - Bitmaps 位存储，可以用 0/1 表示两种状态



#### String

单个 String 存储的最大值为 512 M

##### 场景

用户关注，但也可以用 Hash 来存储

```markdown
set uid:7830298:follow 0
# 关注自增
incr uid:7830298:follow
```



##### 用法

###### 设置/取出

```markdown
127.0.0.1:6379> set a 100
OK
127.0.0.1:6379> type a
string
127.0.0.1:6379> STRLEN a
(integer) 3
127.0.0.1:6379> append a "hello world"
(integer) 14
127.0.0.1:6379> get a
"100hello world"
127.0.0.1:6379> strlen a
(integer) 14
127.0.0.1:6379> 

## 先获取后设置
# 会先返回当前值，而后设置。没有则返回null，但会设置新值
getset a "mongodb"
```

###### 设置多个值

```markdown
## mset 是一个原子操作，要么一起成功，要么一起失败
# mset key value key1 value2
mset k1 v1 k2 v2 

# 若不存在，则创建
msetnx k1 k2 k4 v4
```

###### 自增/自减

```markdown
# 自增
incr count
# 自减
decr count
# 增加步长
incr count 10
```

###### 截取/替换

```markdown
# 截取字符串 取0~3的字符串 xxxx
getrange count 0 3
getrange count 0 -1 # 截取全部
# 替换
setrange count 0 hello
```

###### 超时处理

```markdown
# 设置超过多久后设置成
# setex(set with expire) 设置过期时间
# setnx(set if not exist) 不存在则设置，在分布式锁中经常使用(乐观锁)
# 设置a为 hello, 且在30秒后过期
setex a 30 "hello"
# 如果a不存在则增加一个a=100
setnx a 100
```

###### 对象处理

```markdown
# 简单的 redis 对象设置
set user:1 {name:zhangsan,age:3}

# 快速设置
set user:1:name tom
mset user:1:name helen user:1:age 18
mget user:1:name user:1:age
```



#### List

##### 场景

缓存队列



##### 用法

可以将 List 看成一个双向队列

- 向队列头部添加，则需要在命令最前面加上 `l`
- 向队列尾部添加，则需要在命令最前面加上 `r`

```markdown
########## 插入 ##########
## 向头部插入 [three,two,one]
lpush list one
lpush list two
lpush list three

# 向尾部插入 [one,two,three]
rpush list one
rpush list two
rpush list three

# 在某个元素之前或之后插入
linsert list before four 4
linsert list after four 4

########## 移除 ##########
# 头部出栈
lpop list

# 尾部出栈
rpop list

### 精确移除某个值
# 移除一个 one
lrem [对象] [个数] [精确的值]
lrem list 1 one

########## 截取 ##########
# 注意，ltrim 命令会修改原数组
# ltrim [对象] [起始] [结束]
# 截取0~2位，即0，1,2
ltrim list 0 2

########## 查看 ##########
# 范围取值
lrange list 0 -1

# 根据下标获得 list 的某一个值
lindex list 0

# 查看列表长度
llen list

########## 更新 ##########
# lset [对象] [索引] [要更新的值]
lset list 0 item

########## 排序 ##########
# lset [对象] [索引] [要更新的值]
lset list 0 item

########## 组合命令 ##########
rpoplpush oldlist new list
```





#### Set（无序不重复集合）

##### 场景

如查看两个用户的共同朋友、共同关注，则可以使用交集

如推荐好友，则可以使用差集



##### 用法

```markdown
########## 添加 ##########
sadd myset hello

########## 查看数据 ##########
smembers myset

########## 查看是否存在 ##########
sismember myset hello

########## 获取集合个数 ##########
scard mset

########## 移除某个元素 ##########
srem myset hello

# 随机移除
spop myset 

# 将元素从一个 set 移动到另一个 set
smove oldset newset hello

########## 随机抽取指定个数的元素 ##########
# srandmember [对象] [个数，不指定则为1]
srandmember myset 2

########## 差集/交集/并集 ##########
# 查看差集
sdiff k1 k2
# 查看交集
sinter k1 k2
# 查看并集
sunion k1 k2
```



#### Hash（map 集合）

##### 场景

适合存储对象类数据，如用户数据，以 user:[userid] 作为 key

##### 用法

```markdown
######### 设置值 #########
hset myhash name tom
hset myhash age 18
# 设置多个
hmset myhash name tom age 18
# 如果不存在才创建，存在则创建
hsetnx myhash name tom

# 获取值
hget myhash name
hmget myhash name age

# 删除hash指定的key字段
hdel myhash name

# 查看键值对数量
hlen myhash

# 是否存在某个值
hexists myhash name

# 自增
hincrby myhash age 1
# 自减
hincrby myhash age -1
```



#### Zset（有序集合）

##### 场景

存储成绩、工资等数值排序、排行版、热点列表

##### 用法

在 set 的基础上增加了一个值

```markdown
##### 添加值 #####
# zadd myset [可以对比的数值] [对应的key]
# 可以理解成反着设 key 和 value
zadd myset 1 one
zadd myset 2 two 3 three

##### 查看 #####
# 默认是升序排序
zrange myset 0 -1

##### 排序 #####
# 默认是升序排序
zrange myset 0 -1

# 反序排序
zrevrange salary 0 -1 withscores

# 排序查找负无穷到正无穷的数据，带上 withscores 则显示对应的数值
zrangebyscore salary -inf +inf withscores
# 查找在 0~2500 的数据
zrangebyscore salary 0 2500 withscores
# 查找大于0的数据
zrangebyscore salary 0 2500 withscores

##### 删除节点 #####
zrem salary one

##### 查看个数 #####
zcard salary

##### 获取指定区间的成员数量 #####
# 获取工资在 0~10000 的成员数量
zcount salary 0 10000
```



#### Geospatial（地理位置）

Geo 在 Redis3.2 就推出了，可以推算出地理位置的信息，两地之间的距离

##### 场景

附近的人、周围的人、导航、相距多远等

##### 用法

```markdown
##### 添加地理位置 #####
# geoadd [对象] [经度] [纬度] 名称
# 有效的经度 -180度到180度
# 有效的纬度 -85.05 到 85.05
# 超出范围则会报错
geoadd china:city 116.40 39.90 beijing
geoadd china:city 160.50 29.53 chongqing 114.0 22.52 shenzhen 121.47 31.23 shanghai 120.16 30.24 hangzhou

##### 查看 #####
zrange china:city 0 -1

##### 删除 #####
zrem china:city beijing

##### 查询位置的经纬度 #####
geopos china:city beijing
geopos china:city beijing chongqing hangzhou

##### 查询两个地址的直线距离 #####
# geodist [对象] [地址1] [地址2] [m米/km千米/ml英里/ft英尺]
geodist china:city beijing chongqing km

##### 以给定的经纬度为中心，查找半径内的元素集合 #####
# georadius [对象] [经度] [纬度] [范围数值] [m米/km千米/ml英里/ft英尺]
# 以(110,30)这个经纬度坐标为中心，500千米内的所有成员名
georadius china:city 110 30 500 km
# 查看位置
georadius china:city 110 30 500 km withcoord
# 限制查看个数，限制只看一个
georadius china:city 110 30 500 km withcoord count 1

# 如查看距离，排序等等
http://www.redis.cn/commands/georadius.html

##### 以给定的成员位置为中心，查找半径内的元素集合 #####
getradiusbymember china:city beijing 5000 km
```



#### Hyperloglog（基数）

##### 基数

基数：不重复元素的个数

如 (1,2,3,4,5)   基数 = 5

​     (1,2,2,1,4)   基础 = 3



##### 概念

Hyperloglog 使用了基数统计算法，用于做不重复性的统计

如查看网站的访问数量：

- 传统的计算方式
  - 使用 set 集合存储用户的访问，之后统计 set 集合的数量。但 set 集合的大小会随着保存数据的增长逐渐增大
- 使用 Hyperloglog
  - Hyperloglog 占用的内存是固定的。且错误率仅在 0.81%。从内存角度看是不重复性统计访问量的首选



##### 用法

```markdown
##### 查看 #####
pfadd myhylog a b c d e f g
pfadd myhylog2 e f g h i 7 k

##### 统计基数 #####
pfcount myhylog

##### 合并 #####
pfmerge myhylog myhylog
```



#### Bitmaps（位图）

##### 场景

统计用户信息，如活跃/不活跃、登录/未登录、打卡/未打卡等这类两个状态的，都可以使用 Bitmaps 记录



##### 用法

```markdown
##### 查看 #####
# setbit [对象] [第几位] [值 0/1]
setbit sign 0 1

##### 获取 #####
getbit sign 0

##### 统计 #####
bitcount sign
# 统计0~3位的数据有几个1
bitcount sign 0 3
```



### 事务

`Redis 的单条命令是保证原子性的，但 Redis 的事务是不保证原子性的。因为在编译器异常可以保证所有命令都不能执行，但运行时异常其他正确命令依旧会被执行`

Redis 事务的本质：

- 一组命令的集合，一个事务中的所有命令都会被序列化，在事务执行的过程中都会按照顺序执行
- 所有的命令在事务中都没有被直接执行，而是最后发起执行命令时才会执行

```
## 如队列
------ 事务开启 set set set 执行或取消  --------
```



#### 特点

- 没有隔离级别的概念
  - 所有的命令在事务中并没有直接被执行



#### 使用

- 开启事务(multi)
- 命令入队(...)
- 执行事务(exec)
- 放弃事务(discard)

```markdown
127.0.0.1:6379> multi
OK
127.0.0.1:6379> set k1 v1
QUEUED
127.0.0.1:6379> set k2 v2
QUEUED
127.0.0.1:6379> get k2
QUEUED
127.0.0.1:6379> set k3 v3
QUEUED
127.0.0.1:6379> exec
1) OK
2) OK
3) "v2"
4) OK
```



#### 异常

- 编译型异常
  - 即写入了错误的命令，则事务中所有的命令都不会执行
- 运行时异常
  - 其他命令依然可以执行



#### 监视

https://blog.csdn.net/Evankaka/article/details/70570200 redis 实现乐观锁实现秒杀系统

使用 watch 可以当做 redis 的乐观锁操作，使用 watch 监视一个对象后，

```
# 监视
watch [对象]

# 取消监视
unwatch [对象]
```

<img src="数据库.assets/image-20210225134545500.png" alt="image-20210225134545500" style="zoom:80%;" />



### redis.conf 文件分析

对 redis.conf 文件的内容解析：

#### 网络

```markdown
bind 127.0.0.1  #允许访问的ip
protected-mode yes  # 是否开启保护模式，开启则无法远程访问
port 6379  # 端口设置
```



#### 通用

```markdown
daemonize yes  # 是否以守护进程方式运行
pidfile /var/run/redis_6379.pid   # 若以守护进程方式运行，则需要指定一个pid文件

# 日志级别
# This can be one of:
# debug (a lot of information, useful for development/testing)
# verbose (many rarely useful info, but not a mess like the debug level)
# notice (moderately verbose, what you want in production probably)
# warning (only very important / critical messages are logged)
loglevel notice  # 日志级别，一般生产环境就用 notice
logfile ""  # 日志的位置

databases 16  # 数据库数量

always-show-logo yes  # 是否总是显示logo
```



#### RDB 快照

`持久化：在规定的时间内，执行了多少次操作，会持久化到文件 .rdb.aof`

```markdown
# 如果 900 秒内至少有 1 个 key 修改，就进行持久化操作
save 900 1
# 如果 300 秒内至少有 10 个 key 修改，就进行持久化操作
save 300 10
# 如果 60 秒内至少有 10000 个 key 修改，就进行持久化操作
save 60 10000

# 是否压缩 rdb 文件，会消耗一定cpu资源
rdbcompression yes
# 保存 rdb 文件是否会进行错误的检查校验
rdbchecksum yes
# rdb 文件的保存路径
dir ./
# 设置 dump 文件的文件名
dbfilename dump.rdb
```



#### 安全

```markdown
# 设置登录密码
requirepass # 添加后客户端登录要加上 auth
```



#### 限制

```markdown
# 限制最大的客户端连接数量，默认注释
maxclients 10000
```



#### 内存

```markdown
# 设置内存的最大容量
maxmemory <bytes>
# 内存达到上限后的处理策略
maxmemory-policy noeviction
	volatile-lru(默认)：只对设置了过期时间的key进行LRU
	allkeys-lru：删除lru算法的key
	volatile-random：随机删除即将过期的key
	allkeys-random：随机删除key
	volatile-ttl：删除即将过期的key
	noeviction：永不过期，返回错误
```



#### AOF 相关

```markdown
# 默认不开启aof模式，默认使用rdb方式持久化。在大多数情况系，rdb完全够用
appendonly no
# aof持久化文件的配置
appendfilename "appendonly.aof"

# aof策略
appendfsync everysec
	everysec 每秒执行一次sync，但可能会丢失这一秒的数据
	always 每次修改都会同步，会消耗更多性能
	no 不执行，等同于关闭aof
```



### 持久化

Redis 是内存数据库，如果不将内存中的数据库状态保存到磁盘中，一旦服务器进程退出，服务器中的数据库状态也会消失。因此 Redis 提供了持久化功能。



#### 详细资料

https://blog.csdn.net/zx711166/article/details/82970605

https://blog.csdn.net/ctwctw/article/details/105147277?utm_source=app



#### RDB（Redis Databases Redis备份）

##### 概念

` 在指定的间隔时间内，将内存中的数据集快照写入到磁盘中，可以当做是 Snapshot 快照。当要恢复数据时，直接将快照文件读取到内存中。一般默认使用的也是RDB模式`



##### 流程

- Redis 父进程先判断当前是否有其他子进程在执行，如果有则返回，没有则 fork 一个子进程，此时父进程是阻塞的，Redis 不能执行业务
- 父进程 fork 后，bgsave 命令返回 "Background saving started" 信息，且父进程不再阻塞，Redis 可以处理业务
- 子进程创建临时 rdb 文件，根据父进程内存快照生成临时快照文件，然后对原有文件进行原子替换
- 替换完成后，子进程发送信号给父进程，父进程更新统计信息

![img](数据库.assets/20200731221212551.png)



##### 触发机制

以下几种情况都会触发 rdb 机制

- save 的规则满足的情况下，会自动触发 rdb 规则
- 执行 flushall 命令，也会触发 rdb 规则
- 退出 redis 也会产生 rdb 文件



##### 使用 rdb 文件恢复数据

- 只需要将 rdb 文件放在 redis 的启动目录中即可，redis 启动时会自动检查 dump.rdb 并恢复其中的数据

- 查看的命令，需要先进入客户端

  ```
  config get dir
  ```



##### 优缺点

###### 优点

- 适合大规模的数据恢复

###### 缺点

- 最后一次持久化的数据可能会丢失(即在指定的时间间隔时，进程退出了。这时这部分内存的数据就没有写入到磁盘，就会丢失)



##### 适用场景

适用于大规模的数据恢复，且对数据恢复的完整性不是十分敏感，则 RDB 方式比 AOF 更加高效



#### AOF（Append Only File 文件追加）

##### 概念

`以日志的形式记录每个写操作，将 Redis 执行过的除读操作外的指令都记录下来(如插入、更新、删除等)，只许追加文件不能改写文件。Redis 启动时会读取该文件重新构建数据。即 Redis 重启后会根据日志文件的内容将指令从头到尾执行一次，实现数据恢复工作。Redis支持设置每次写操作都触发或是每秒触发`

<img src="数据库.assets/image-20210225230506925.png" alt="image-20210225230506925" style="zoom:67%;" />



##### 流程

- Redis 父进程先判断当前是否有其他子进程在执行，如果有则返回，没有则 fork 一个子进程，此时父进程是阻塞的，Redis 不能执行业务
- 父进程 fork 后，bgsave 命令返回 "Background saving started" 信息，且父进程不再阻塞，Redis 可以处理业务
- 子进程创建临时 aof 文件，根据父进程内存快照生成临时快照文件，这个过程中 Redis 获取的数据都会被主进程缓存，在子进程创建临时 aof 文件后，写入临时 aof 文件中。最后对原有文件进行原子替换。
- 替换完成后，子进程发送信号给父进程，父进程更新统计信息
- 之后产生的日志数据都会被主进程追加到 AOF 文件中，这时会对主进程造成阻塞



##### 如果aof文件损坏

bin 目录下提供了 `redis-check-aof` 和 `redis-check-rdb` 检测文件，实现自动修复。修复前要关闭 Redis。

```
./bin/redis-check-aof --fix appendonly.aof
```



##### 优缺点

###### 优点

- 每一次修改都同步或每秒都同步一次，丢失的数据更少，文件完整性更好

###### 缺点

- aof 文件占用的空间更大(rdb文件存储的是数据，aof 文件存储的是指令，指令肯定比存储内容更多)
- 运行效率不如 rdb



### 扩容

Redis 中数据存储是通过 dict 字典实现的，内部使用了哈希表存储数据，因此 Redis 扩容其实就是对字典中的哈希表进行 rehash 操作。

- 若 Redis 仅使用主从模式，则是使用`一致性哈希`的方式支持扩容
- Redis 集群模式改为分配 hash 槽(一共16384个槽)的方式进行扩容，增加和删除节点都需要重新分配 hash 槽



### 淘汰策略和过期策略

#### 设置

如果在部署 Redis 时设置的值没有设置过期时间，或是大量并发的情况下，可能导致 Redis 占用的内存越来越多，最终影响到其他服务，因此我们应该在部署时指定 Redis 占用的最大内容容量

```markdown
# 设置内存的最大容量
maxmemory <bytes>

# 内存达到上限后的处理策略
maxmemory-policy noeviction
	volatile-lru(默认)：只对设置了过期时间的key进行LRU
	allkeys-lru：删除lru算法的key
	volatile-random：随机删除即将过期的key
	allkeys-random：随机删除key
	volatile-ttl：删除即将过期的key
	noeviction：永不过期，返回错误
```



#### LRU(最近最少用算法)

`LRU(Least Recently Used ) 算法是一种内存管理算法，它认为：长期不被使用的数据，在未来被用到的几率也不大。因此在数据达到一定阈值时，要移除掉最近最少使用的数据`



#### 过期策略

- **定时过期：**对每个设置了过期时间的 key 都创建一个定时器，到过期时间了就立刻清除
  - 可以立即清除内存中的过期数据，对内存友好。但需要占用大量的 cpu 资源处理过期数据
- **惰性过期：**只有当访问一个 key 时，才会判断当前 key 是否过期，过期则清除
  - 可以最大化节省 cpu 资源，但如果这个key不在被访问就不会被清除，会一直占用内存空间
- **定期过期：**每隔一定时间，就会扫描一定数量的数据库中的expires字典中的一部分key，并清除其中已经过期的key
  - 通过调整定时扫描的间隔时间和每次扫描的限定耗时，在 cpu 资源占用和内存空间资源回收间达到平衡
- `Redis 采用惰性过期和定期过期结合的方式`



### 发布订阅

`Redis 发布/订阅是一种消息通信模式，发送者发送消息，订阅者接收消息。Redis 客户端可以订阅任意数量的频道。`



#### 原理

在 redis 中维护了一个字典，字典的键就是一个个频道，值是一个链表，链表中存储了所有订阅了这个频道的客户端。而发布信息时，redis 会根据订阅的频道从字典中取得订阅者链表，遍历发送。依此可以实现推送、聊天功能、关注系统等。



#### 命令

```
# 订阅频道
sucscribe group001

# 发布信息到频道
publish group001 hellofriend
```





### 主从复制 / 哨兵模式

#### 主从复制

##### 概念

`Redis 的主从复制就是将一台 Redis(主节点/Master) 的数据复制到其他 Redis 服务器(从节点/Slave)。数据的复制是单向的，只能从主节点到副节点。Master 以写为主，Slave 以读为主。一般至少配置一主二从。 `



##### 作用

- **数据冗余：**实现了数据的热备份，是持久化外的另一种数据冗余方式
- **故障恢复：**当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复
- **负载均衡：**在主从复制的基础上实现读写分离，由主节点提供写服务，从节点提供读服务(写redis数据时连接主节点，读redis数据时连接从节点)，分担负载，尤其是读多写少的情况下(如电商)，多个从节点分担读压力，能够大大提高性能
- **高可用：**主从复制是哨兵模式和集群模式的基础



##### 原理

- 当从机成功连接到主机后，会向主机发送一个 sync 同步命令
- Master 接收到命令后，会启动后台的存盘进程，同时收集所有接受到的用于修改数据集的命令，在后台进程执行完毕前，master 将传送整个数据文件到 slave
- slave 在接收到 master 传来的数据文件后，会将其存盘并加载到内存中，实现一次全量复制
- master 继续将新的修改命令发送给 slave，保持同步，这就是增量复制
- 每次 slave 重连到 master，都会进行一次全量复制



#### 哨兵模式(Sentinel) 

https://www.cnblogs.com/buttercup/p/14051301.html

`哨兵模式是 redis 提供的一个高可用方案，是一个独立的进程，可以监督 redis 主从的节点状态，实现故障时主从自动切换以及故障通知。原理是通过发送 Redis 命令给服务器来监控服务器的运行状态`



##### 作用

- 通过发送命令来监控主从的节点状态
- 在 master 宕机时，自动主从切换，快速恢复故障环境
- 可以在发生故障后，通过定义好的脚本通知运维人员



##### 故障切换流程

故障切换分为`主观下线`和`客观下线`两个流程

- 主观下线
  - 当一个`哨兵节点A`发现 Master 宕机后并不会马上进行 fallover 过程，而只是`哨兵节点A`认为这个 Master 不可用
- 客观下线
  - 当其他的哨兵节点也发现 Master 宕机，且发现的哨兵节点数量到达半数以上时，哨兵间就会发起投票选出新的 master 节点，投票的结果由一个哨兵发起，进行 failover。当新的 master 切换成功后，会通过发布/订阅模式，让哨兵经自己监控的 slave 切换成 master



##### 优缺点

- 优点
  - 哨兵支持集群，避免哨兵出现问题
  - 实现了主从自动切换，故障转移，能够让故障快速恢复，提高系统可用性
  - 发生故障时主动通知
- 缺点
  - 难以在线扩容，集群容量一旦到达上限，在线扩容很麻烦
  - 配置麻烦



##### 配置详解

sentinel.conf

```markdown
# 哨兵端口
port 26379

# 是否后台启动
daemonize no

# pid 文件位置
pidfile /var/run/redis-sentinel.pid

# 哨兵日志文件名
logfile ""
# 日志地址
dir /tmp

# 设置监听的 redis 服务
# sentinel monitor [自定义名称] [服务host] [服务端口号] [是启动故障转移failover时，所需的最小哨兵数量]
sentinel monitor mymaster 127.0.0.1 6379 1

# 在多少毫秒后，主节点没有应答哨兵则哨兵主观认为主节点下线，默认30s
sentinel down-after-milliseconds mymaster 30000

# 通知脚本，即当 sentinel 有任何警告级别的事件发生时，回去调用这个脚本。
# 这个脚本中可以事件以邮件或SMS等方式去通知提醒管理员
# 会提供两个参数，一个事件的类型，另一个是事件的描述
sentinel notification-script mymaster /var/redis/notify.sh
```



#### 环境搭建(一主二从三哨兵)

##### 下载

http://download.redis.io/releases/redis-6.0.6.tar.gz

记得先看单节点部署，才能部署多节点



##### 单节点部署

###### 安装和运行

安装 gcc 依赖

```
yum install -y gcc 
gcc -v

#若redis版本6.0.x，则gcc要升级到 5.3及以上版本
yum -y install centos-release-scl
yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils
scl enable devtoolset-9 bash
```

下载安装包，解压重命名

```
wget http://download.redis.io/releases/redis-5.0.3.tar.gz
tar -zxvf redis-5.0.3.tar.gz
mv redis-5.0.3.tar.gz redis
```

执行编译和安装

```
cd /opt/redis
make
make install PREFIX=/opt/redis
```



###### 5.1.2 启动脚本和关闭脚本

启动脚本

````
vim /opt/redis/start.sh

# 脚本内容
./bin/redis-server ./redis.conf
````

关闭脚本

```
vim /opt/redis/stop.sh

# 脚本内容
./bin/redis-cli shutdown
```

修改脚本执行权限

```
chmod +711 start.sh stop.sh
```

运行

```
sh start.sh
```



###### 设置后台运行

修改配置文件，修改后重启 redis

````
vim /opt/redis/redis.conf
修改 daemonize no 为 yes
````



###### 设置远程访问

```
# 将 bind 注释掉
#bind 127.0.0.1
```



###### 修改端口和密码

```
vim /opt/redis/redis.conf
```

修改默认端口

```
将 port 6379  改为自己想要的端口
```

设置redis的密码

```
# 在配置文件中找到 requirepass
requirepass 后改为自己的密码
```

访问

```
./opt/redis/bin/redis-cli -p [端口号] -a [密码] 

# 其他操作
./opt/redis/bin/redis-cli --help
```



##### 一主双从配置

###### 修改配置

主从其实就是将 redis 配置文件复制出三份，然后对 redis.conf 进行配置。

其他的另个从节点只要把 7001 改为 7002 或是 7003 即可

```markdown
###### 网络相关配置 ######
# 设置仅允许内网访问
# bind 172.0.34.1
# 设置 redis 端口
port 7001

###### 通用相关配置 ######
# 是否以守护进程方式运行
daemonize yes  
# 若以守护进程方式运行，则需要指定一个pid文件，文件名后面加上自己的端口号
pidfile /var/run/redis7001.pid   

###### 日志相关配置 ######
# 日志级别，一般生产环境就用 notice
loglevel notice  
# 日志的位置
logfile "/opt/redis-c/redis7001.log"  

###### rdb相关配置 ######
# rdb 文件的保存路径
dir "/opt/redis-c/redis7001"
# 设置 dump 文件的文件名
dbfilename "dump7001.rdb"

###### 安全相关配置 ######
# 设置登录密码
requirepass mypassword # 添加后客户端登录要加上 auth

###### 主从相关配置 ######
# 配置主节点地址 ip+端口号
replicaof www.xxx.top 7001

# 配置主节点访问密码，如果没有则注释掉
masterauth tomtom
```



###### 编写脚本

集群启动脚本

```markdown
./redis7001/bin/redis-server ./redis7001/redis.conf
./redis7002/bin/redis-server ./redis7002/redis.conf
./redis7003/bin/redis-server ./redis7003/redis.conf

ps -ef|grep redis
```

集群关闭脚本

```
./redis7001/bin/redis-cli -p 7001 shutdown 2>/dev/null
./redis7002/bin/redis-cli -p 7002 shutdown 2>/dev/null
./redis7003/bin/redis-cli -p 7003 shutdown 2>/dev/null

ps -ef|grep redis
```

客户端访问脚本

```
./redis7001/bin/redis-cli -p 7001
```



###### 相关命令

```markdown
# 查看主从信息
info replication

# 手动作为从节点添加到 master 去
slaveof [主机host] [主机端口]

# 当发生异常时，手动将当前节点置为 master
slaveof no one
```



##### 三哨兵集群

###### 修改配置

修改 sentinel.conf 配置文件，其他哨兵节点只要改端口号以及对应的文件名称即可

```markdown
###### 网络相关配置 ######
# 哨兵占用的端口号
port 26379

###### 通用相关配置 ######
# 后台运行
daemonize yes
# pid 文件名，后台运行模式下必须填
pidfile "sentinel26379.pid"

###### 日志相关配置 ######
# 日志文件存储路径和名称，用于查看监控情况和选举情况
logfile "/opt/redis-c/redis-sentinel/log/sentinel26379.log"
# 定义存储目录，对pid文件生效
dir "/opt/redis-c/redis-sentinel/log"
# 哨兵的身份id，只能为 A-z 0-9 .-_，若哨兵集群则需要保证 myid 唯一
sentinel myid 2875a5f2053ba647c60f928158ad75621affabfd

###### 连接相关配置 ######
# sentinel minitor [哨兵集群名称] [redis节点master的host] [redis节点master的ip] [最少投票的哨兵数]
# 注意，当实际哨兵数小于设置的最少投票哨兵数，则不会再自动切换主从。master 宕机后其余节点一直是 slave，redis 服务仅能提供读操作
sentinel monitor mymaster www.xxx.top 7001 2
```



###### 编写脚本

启动脚本

```markdown
./bin/redis-sentinel sentinel26379.conf 
./bin/redis-sentinel sentinel26380.conf 
./bin/redis-sentinel sentinel26381.conf 
```

关闭脚本

```markdown
NAME='sentinel'
PID=$(ps -ef |grep $NAME | grep -v grep | awk '{print $2}'|awk -F '/' '{print $1}')
if [ -n "$PID" ]; then
        kill -9 $PID
        echo kill $PID successful
else
        echo kill failed
fi
ps -ef|grep $NAME
```

客户端访问脚本

```markdown
./bin/redis-cli -h 127.0.0.1 -p 26379
```



###### 相关命令

```markdown
# 访问哨兵
./bin/redis-cli -h 127.0.0.1 -p 23679
# 查看哨兵集群状态
info sentinel
```



##### 命令总结

```markdown
########## 主从 ##########
# 查看主从节点状态
info replication

# 添加为从机
# slaveof [主机host] [主机端口]
slaveof 127.0.0.1 7001

# 当出现问题，手动置为主机
slaveof no one

########## 哨兵 ##########
# 访问哨兵客户端
./bin/redis-cli -h 127.0.0.1 -p 23679
# 查看哨兵集群状态
info sentinel
```



### 缓存穿透、缓存击穿、缓存雪崩

#### 缓存穿透

##### 概念

`当用户想要查询一个数据，结果redis中没有，即缓存没有命中，于是向持久层数据库查询，发现也没有，于是本次查询失败。如果大量这种查询失败的情况发生，则会对数据库造成很大压力。这就是缓存穿透`



##### 解决方案

###### 布隆过滤器

`布隆过滤器是一种数据结构，对所有可能查询的参数以 hash 形式存储，在控制层先进行校验，不符合则丢弃，从而避免对底层存储系统的查询压力`

![image-20210227114046650](数据库.assets/image-20210227114046650.png)



###### 缓存空对象

当持久层数据库也没有命中时，即使数据库返回空对象也将其缓存起立，同时设置一个国企事件，之后再次访问这个数据将会从缓存中获取，避免大量无意义的查询



**问题：**

- 将空值缓存起来，大量并发下可能会出现很多空值的键
- 即使对空值设置了过期时间，还是会存在数据不一致的情况，对一致性的业务会有影响



#### 缓存击穿

##### 概念

`缓存穿透是指某个热点key在失效的瞬间，如 key 过期，此时大量并发请求就无法在缓存中命中，即穿透缓存，导致数据库瞬间压力过大。`



##### 解决方案

###### 设置热点数据永不过期

如果热点数据没有过期时间，就不会出现过期的问题



###### 分布式锁+双重校验

缓存击穿的原理是大量并发同时对一个key进行查询。我们可以使用分布式锁+双重锁校验，保证每个key同时只有一个线程去查询，其他线程等待。





#### 缓存穿透和缓存击穿的区别

- 范围不同
  - 缓存穿透是大面积的缓存未命中，而缓存击穿则是对热点key的缓存未命中
- 持续时间不同
  - 缓存穿透的影响是持续性的
    - 缓存穿透是在缓存中和数据库中都查询不到，这时就会造成持续性的穿透，对数据库和缓存都造成大量并发压力
  - 缓存击穿的影响可能是瞬间的，也可能是持续的
    - 缓存击穿则是在缓存中查询不到，去查询数据库，对数据库造成大量并发压力。若数据库中查询到了，则之后会更新缓存数据，之后的访问可以直接在缓存中命中。若查询不到，会造成持续性的并发压力



#### 缓存雪崩

##### 概念

`缓存雪崩指某一个时间段内，缓存集中过期失效或redis宕机，导致大量并发请求同一时间访问数据库，造成数据库并发压力增大，造成数据库崩溃。`

缓存集中过期可能导致数据库周期性的压力增大，大量并发的同时也会更新大批量的key到缓存中。但如果缓存服务宕机，则会导致数据库持续性的压力。

<img src="数据库.assets/image-20210227122130895.png" alt="image-20210227122130895" style="zoom:67%;" />



##### 解决方案

- redis 高可用

- 服务限流和降级
  - 在缓存失效后，通过加锁或控制访问数据库的线程数量，降低并发请求
- 数据预热
  - 即在正式部署之前，先手动触发加载不同的key到redis中，且设置不同的过期时间，让缓存失效的时间点尽量均匀



### 集群部署

#### ruby 环境安装(redis版本>=5.0则忽略)

安装 ruby

```
yum install ruby
ruby -v
```

**若 ruby 版本过低，则使用 rvm 来更新 ruby**

先删除原来的库，改用阿里云镜像

```
# 删除原本的仓库地址
gem sources --remove https://rubygems.org/

# 安装阿里云镜像
gem sources -a http://mirrors.aliyun.com/rubygems/ 
```

安装 RAM（[Ruby Version Manager](https://rvm.io/) ）

```
gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB
 
curl -sSL https://get.rvm.io | bash -s stable
```

**若提示：curl: (7) Failed to connect to raw.githubusercontent.com port 443: Connection refused**

```
# 这是由于某些原因导致域名污染，设置下 host
vim /etc/hosts

# 去 https://www.ipaddress.com/ 查这个域名，并增加一条对应内容，修改后重新执行上面的命令即可
199.232.96.133  raw.githubusercontent.com
```

更新配置，使其马上生效

```
source /etc/profile.d/rvm.sh
```

查看可用版本，下载和更新版本

```
# 查看可用版本
rvm list known

# 更新版本，redis要求 ruby 版本大于 2.2.2
rvm install 2.5

# 重新查看版本
ruby -v
```



#### Redis 集群配置(三主三从)

###### 配置

将 redis 文件夹复制6份，对应端口分别为： 7001 ~ 7006

```
mkdir /opt/redis-cluster
cd /opt/redis-cluster
cp -a ../redis ./redis-7001
cp -a redis-7001/ redis-7002
cp -a redis-7001/ redis-7003
cp -a redis-7001/ redis-7004
cp -a redis-7001/ redis-7005
cp -a redis-7001/ redis-7006
```

修改配置文件

```
vim redis.conf

# 打开配置
cluster-enable yes
# 修改对应端口
port 7001
# 关闭保护模式
#bind 127.0.0.1
protected-mode no
```



###### 启动/关闭脚本

编写启动脚本

```
# 启动脚本
sh start.sh

# 脚本内容
cd redis-7001
./bin/redis-server redis.conf
cd ..
cd redis-7002
./bin/redis-server redis.conf
cd ..
cd redis-7003
./bin/redis-server redis.conf
cd ..
cd redis-7004
./bin/redis-server redis.conf
cd ..
cd redis-7005
./bin/redis-server redis.conf
cd ..
cd redis-7006
./bin/redis-server redis.conf
cd ..
 
sleep 2
ps -ef|grep redis
```

编写关闭脚本

```
cd redis-7001
./bin/redis-cli -p 7001 (-a [对应密码]) shutdown 2>/dev/null
cd ..
cd redis-7002
./bin/redis-cli -p 7002 (-a [对应密码]) shutdown 2>/dev/null
cd ..
cd redis-7003
./bin/redis-cli -p 7003 (-a [对应密码]) shutdown 2>/dev/null
cd ..
cd redis-7004
./bin/redis-cli -p 7004 (-a [对应密码]) shutdown 2>/dev/null
cd ..
cd redis-7005
./bin/redis-cli -p 7005 (-a [对应密码]) shutdown 2>/dev/null
cd ..
cd redis-7006
./bin/redis-cli -p 7006 (-a [对应密码]) shutdown 2>/dev/null
cd ..
 
sleep 2
ps -ef|grep redis
```



###### **vRedis < 5.0**

获取官方提供的集群文件

```
cp /opt/redis/src/redis-trib.rb /opt/redis-cluster
```

启动集群

```
# 先清理旧数据，否则会报错
rm -rf ./redis-7001/nodes.conf ./redis-7001/dump.rdb
rm -rf ./redis-7002/nodes.conf ./redis-7002/dump.rdb
rm -rf ./redis-7003/nodes.conf ./redis-7003/dump.rdb
rm -rf ./redis-7004/nodes.conf ./redis-7004/dump.rdb
rm -rf ./redis-7005/nodes.conf ./redis-7005/dump.rdb
rm -rf ./redis-7006/nodes.conf ./redis-7006/dump.rdb

./redis-trib.rb create --replicas 1 47.111.111.111:7001 47.111.111.111:7002 47.111.111.111:7003 47.111.111.111:7004 47.111.111.111:7005 47.111.111.111:7006 (-a [密码])
```



###### vRedis >= 5.0

若 redis 版本大于等于5.0，则不再使用ruby 语言，直接接用

```
# 先清理旧数据，否则会报错
rm -rf ./redis-7001/nodes.conf ./redis-7001/dump.rdb
rm -rf ./redis-7002/nodes.conf ./redis-7002/dump.rdb
rm -rf ./redis-7003/nodes.conf ./redis-7003/dump.rdb
rm -rf ./redis-7004/nodes.conf ./redis-7004/dump.rdb
rm -rf ./redis-7005/nodes.conf ./redis-7005/dump.rdb
rm -rf ./redis-7006/nodes.conf ./redis-7006/dump.rdb

./redis-cli --cluster create 47.111.111.111:7001 47.111.111.111:7002 47.111.111.111:7003 47.111.111.111:7004 47.111.111.111:7005 47.111.111.111:7006 --cluster-replicas 1 (-a [密码])
```



#### 整合： 集群启动脚本

```
# 创建集群启动脚本
vim cluster-start.sh

# 脚本内容
rm -rf ./redis-7001/nodes.conf ./redis-7001/dump.rdb
rm -rf ./redis-7002/nodes.conf ./redis-7002/dump.rdb
rm -rf ./redis-7003/nodes.conf ./redis-7003/dump.rdb
rm -rf ./redis-7004/nodes.conf ./redis-7004/dump.rdb
rm -rf ./redis-7005/nodes.conf ./redis-7005/dump.rdb
rm -rf ./redis-7006/nodes.conf ./redis-7006/dump.rdb

sh start.sh

sleep 2s
./redis-cli --cluster create 47.114.177.22:7001 47.114.177.22:7002 47.114.177.22:7003 47.114.177.22:7004 47.114.177.22:7005 47.114.177.22:7006 --cluster-replicas 1 -a Chat#*12
```



#### 命令

查看某个节点信息

```
./redis-cli -p 7002 -a Chat#*12 cluster info 2>/dev/null
```

查看集群的节点情况

```
./redis-cli -p 7001 -a Chat#*12 cluster nodes 2>/dev/null
```



### 结合项目

#### Springboot 连接 Redis 主从集群(哨兵)

##### 增加配置文件

Springboot 通过 jedis(lettcue) 连接到主节点或从节点，但这样设置时，当 Master 宕机，哨兵主从切换后，Jedis 无法实现自动切换。因此应该在配置文件中连接哨兵

```markdown
# 设置最大线程数
spring.redis.lettuce.pool.max-active=20
# 设置最大空闲连接数
spring.redis.lettuce.pool.max-idle=10
# 设置最小空闲连接数
spring.redis.lettuce.pool.min-idle=1
# 设置最长等待时间
spring.redis.lettuce.pool.max-wait=6000

### 这是直接连接，本次用哨兵连接，这部分不用
#spring.redis.host=www.gtom.top
#spring.redis.port=7001

# 哨兵连接
spring.redis.sentinel.master=mymaster
spring.redis.sentinel.nodes=www.gtom.top:26379,www.gtom.top:26380,www.gtom.top:26381
```



##### 增加配置文件 RedisConfig.java

```java
@Configuration
public class RedisConfig {

    @Bean
    @SuppressWarnings("all")
    public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) {
        RedisTemplate<String, Object> redisTemplate = new RedisTemplate<String, Object>();
        redisTemplate.setConnectionFactory(redisConnectionFactory);

        // json 序列化配置
        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);
        ObjectMapper om = new ObjectMapper();
        om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);
        om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);
        jackson2JsonRedisSerializer.setObjectMapper(om);

        // string 的序列化
        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();
        // 设置序列化方式
        // 默认的jdk
        // key 用 string 序列化，不加“”,value用json序列化速度最快
        redisTemplate.setKeySerializer(stringRedisSerializer);
        redisTemplate.setValueSerializer(jackson2JsonRedisSerializer);
        redisTemplate.setHashKeySerializer(stringRedisSerializer);
        redisTemplate.setHashValueSerializer(jackson2JsonRedisSerializer);
        redisTemplate.afterPropertiesSet();
        return redisTemplate;
    }
}
```



##### 封装 JedisTemplate

由于 JedisTemplate 使用麻烦，一般我们都会自己封装

```java
/**
 * Redis Util
 * 操作字符串：redisTemplate.opsForValue();
 * 操作hash：redisTemplate.opsForHash();
 * 操作list：redisTemplate.opsForList();
 * 操作set：redisTemplate.opsForSet();
 * 操作有序set：redisTemplate.opsForZSet();
 */
@Component("RedisUtils")
public class RedisUtils {

    @Autowired
    private RedisTemplate<String, Object> redisTemplate;

    /**
     * 指定缓存失效时间
     *
     * @param key
     *            键
     * @param time
     *            时间(秒)
     * @return
     */
    public boolean expire(String key, long time) {
        try {
            if (time > 0) {
                redisTemplate.expire(key, time, TimeUnit.SECONDS);
            }
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 根据key 获取过期时间
     *
     * @param key
     *            键 不能为null
     * @return 时间(秒) 返回0代表为永久有效
     */
    public long getExpire(String key) {
        return redisTemplate.getExpire(key, TimeUnit.SECONDS);
    }

    /**
     * 判断key是否存在
     *
     * @param key
     *            键
     * @return true 存在 false不存在
     */
    public boolean hasKey(String key) {
        try {
            return redisTemplate.hasKey(key);
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 删除缓存
     *
     * @param key
     *            可以传一个值 或多个
     */
    @SuppressWarnings("unchecked")
    public void del(String... key) {
        if (key != null && key.length > 0) {
            if (key.length == 1) {
                redisTemplate.delete(key[0]);
            } else {
                redisTemplate.delete((Collection<String>) CollectionUtils.arrayToList(key));
            }
        }
    }

    // ============================String=============================
    /**
     * 普通缓存获取
     *
     * @param key
     *            键
     * @return 值
     */
    public Object get(String key) {
        return key == null ? null : redisTemplate.opsForValue().get(key);
    }

    /**
     * 普通缓存放入
     *
     * @param key
     *            键
     * @param value
     *            值
     * @return true成功 false失败
     */
    public boolean set(String key, Object value) {
        try {
            redisTemplate.opsForValue().set(key, value);
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }

    }

    /**
     * 普通缓存放入并设置时间
     *
     * @param key
     *            键
     * @param value
     *            值
     * @param time
     *            时间(秒) time要大于0 如果time小于等于0 将设置无限期
     * @return true成功 false 失败
     */
    public boolean set(String key, Object value, long time) {
        try {
            if (time > 0) {
                redisTemplate.opsForValue().set(key, value, time, TimeUnit.SECONDS);
            } else {
                set(key, value);
            }
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 递增
     *
     * @param key
     *            键
     * @param delta
     *            要增加几(大于0)
     * @return
     */
    public long incr(String key, long delta) {
        if (delta < 0) {
            throw new RuntimeException("递增因子必须大于0");
        }
        return redisTemplate.opsForValue().increment(key, delta);
    }

    /**
     * 递减
     *
     * @param key
     *            键
     * @param delta
     *            要减少几(小于0)
     * @return
     */
    public long decr(String key, long delta) {
        if (delta < 0) {
            throw new RuntimeException("递减因子必须大于0");
        }
        return redisTemplate.opsForValue().increment(key, -delta);
    }

    // ================================Map=================================
    /**
     * HashGet
     *
     * @param key
     *            键 不能为null
     * @param item
     *            项 不能为null
     * @return 值
     */
    public Object hget(String key, String item) {
        return redisTemplate.opsForHash().get(key, item);
    }

    /**
     * 获取hashKey对应的所有键值
     *
     * @param key
     *            键
     * @return 对应的多个键值
     */
    public Map<Object, Object> hmget(String key) {
        return redisTemplate.opsForHash().entries(key);
    }

    /**
     * HashSet
     *
     * @param key
     *            键
     * @param map
     *            对应多个键值
     * @return true 成功 false 失败
     */
    public boolean hmset(String key, Map<String, Object> map) {
        try {
            redisTemplate.opsForHash().putAll(key, map);
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * HashSet 并设置时间
     *
     * @param key
     *            键
     * @param map
     *            对应多个键值
     * @param time
     *            时间(秒)
     * @return true成功 false失败
     */
    public boolean hmset(String key, Map<String, Object> map, long time) {
        try {
            redisTemplate.opsForHash().putAll(key, map);
            if (time > 0) {
                expire(key, time);
            }
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 向一张hash表中放入数据,如果不存在将创建
     *
     * @param key
     *            键
     * @param item
     *            项
     * @param value
     *            值
     * @return true 成功 false失败
     */
    public boolean hset(String key, String item, Object value) {
        try {
            redisTemplate.opsForHash().put(key, item, value);
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 向一张hash表中放入数据,如果不存在将创建
     *
     * @param key
     *            键
     * @param item
     *            项
     * @param value
     *            值
     * @param time
     *            时间(秒) 注意:如果已存在的hash表有时间,这里将会替换原有的时间
     * @return true 成功 false失败
     */
    public boolean hset(String key, String item, Object value, long time) {
        try {
            redisTemplate.opsForHash().put(key, item, value);
            if (time > 0) {
                expire(key, time);
            }
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 删除hash表中的值
     *
     * @param key
     *            键 不能为null
     * @param item
     *            项 可以使多个 不能为null
     */
    public void hdel(String key, Object... item) {
        redisTemplate.opsForHash().delete(key, item);
    }

    /**
     * 判断hash表中是否有该项的值
     *
     * @param key
     *            键 不能为null
     * @param item
     *            项 不能为null
     * @return true 存在 false不存在
     */
    public boolean hHasKey(String key, String item) {
        return redisTemplate.opsForHash().hasKey(key, item);
    }

    /**
     * hash递增 如果不存在,就会创建一个 并把新增后的值返回
     *
     * @param key
     *            键
     * @param item
     *            项
     * @param by
     *            要增加几(大于0)
     * @return
     */
    public double hincr(String key, String item, double by) {
        return redisTemplate.opsForHash().increment(key, item, by);
    }

    /**
     * hash递减
     *
     * @param key
     *            键
     * @param item
     *            项
     * @param by
     *            要减少记(小于0)
     * @return
     */
    public double hdecr(String key, String item, double by) {
        return redisTemplate.opsForHash().increment(key, item, -by);
    }

    // ============================set=============================
    /**
     * 根据key获取Set中的所有值
     *
     * @param key
     *            键
     * @return
     */
    public Set<Object> sGet(String key) {
        try {
            return redisTemplate.opsForSet().members(key);
        } catch (Exception e) {
            e.printStackTrace();
            return null;
        }
    }

    /**
     * 根据value从一个set中查询,是否存在
     *
     * @param key
     *            键
     * @param value
     *            值
     * @return true 存在 false不存在
     */
    public boolean sHasKey(String key, Object value) {
        try {
            return redisTemplate.opsForSet().isMember(key, value);
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 将数据放入set缓存
     *
     * @param key
     *            键
     * @param values
     *            值 可以是多个
     * @return 成功个数
     */
    public long sSet(String key, Object... values) {
        try {
            return redisTemplate.opsForSet().add(key, values);
        } catch (Exception e) {
            e.printStackTrace();
            return 0;
        }
    }

    /**
     * 将set数据放入缓存
     *
     * @param key
     *            键
     * @param time
     *            时间(秒)
     * @param values
     *            值 可以是多个
     * @return 成功个数
     */
    public long sSetAndTime(String key, long time, Object... values) {
        try {
            Long count = redisTemplate.opsForSet().add(key, values);
            if (time > 0) {
                expire(key, time);
            }
            return count;
        } catch (Exception e) {
            e.printStackTrace();
            return 0;
        }
    }

    /**
     * 获取set缓存的长度
     *
     * @param key
     *            键
     * @return
     */
    public long sGetSetSize(String key) {
        try {
            return redisTemplate.opsForSet().size(key);
        } catch (Exception e) {
            e.printStackTrace();
            return 0;
        }
    }

    /**
     * 移除值为value的
     *
     * @param key
     *            键
     * @param values
     *            值 可以是多个
     * @return 移除的个数
     */
    public long remove(String key, Object... values) {
        try {
            Long count = redisTemplate.opsForSet().remove(key, values);
            return count;
        } catch (Exception e) {
            e.printStackTrace();
            return 0;
        }
    }
    // ===============================list=================================

    /**
     * 获取list缓存的内容
     *
     * @param key
     *            键
     * @param start
     *            开始
     * @param end
     *            结束 0 到 -1代表所有值
     * @return
     */
    public List<Object> lGet(String key, long start, long end) {
        try {
            return redisTemplate.opsForList().range(key, start, end);
        } catch (Exception e) {
            e.printStackTrace();
            return null;
        }
    }

    /**
     * 获取list缓存的长度
     *
     * @param key
     *            键
     * @return
     */
    public long lGetListSize(String key) {
        try {
            return redisTemplate.opsForList().size(key);
        } catch (Exception e) {
            e.printStackTrace();
            return 0;
        }
    }

    /**
     * 通过索引 获取list中的值
     *
     * @param key
     *            键
     * @param index
     *            索引 index>=0时， 0 表头，1 第二个元素，依次类推；index<0时，-1，表尾，-2倒数第二个元素，依次类推
     * @return
     */
    public Object lGetIndex(String key, long index) {
        try {
            return redisTemplate.opsForList().index(key, index);
        } catch (Exception e) {
            e.printStackTrace();
            return null;
        }
    }

    /**
     * 将list放入缓存
     *
     * @param key
     *            键
     * @param value
     *            值
     * @return
     */
    public boolean lSet(String key, Object value) {
        try {
            redisTemplate.opsForList().rightPush(key, value);
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 将list放入缓存
     *
     * @param key
     *            键
     * @param value
     *            值
     * @param time
     *            时间(秒)
     * @return
     */
    public boolean lSet(String key, Object value, long time) {
        try {
            redisTemplate.opsForList().rightPush(key, value);
            if (time > 0) {
                expire(key, time);
            }
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 将list放入缓存
     *
     * @param key
     *            键
     * @param value
     *            值
     * @return
     */
    public boolean lSet(String key, List<Object> value) {
        try {
            redisTemplate.opsForList().rightPushAll(key, value);
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 将list放入缓存
     *
     * @param key
     *            键
     * @param value
     *            值
     * @param time
     *            时间(秒)
     * @return
     */
    public boolean lSet(String key, List<Object> value, long time) {
        try {
            redisTemplate.opsForList().rightPushAll(key, value);
            if (time > 0) {
                expire(key, time);
            }
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 根据索引修改list中的某条数据
     *
     * @param key
     *            键
     * @param index
     *            索引
     * @param value
     *            值
     * @return
     */
    public boolean lUpdateIndex(String key, long index, Object value) {
        try {
            redisTemplate.opsForList().set(key, index, value);
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

    /**
     * 移除N个值为value
     *
     * @param key
     *            键
     * @param count
     *            移除多少个
     * @param value
     *            值
     * @return 移除的个数
     */
    public long lRemove(String key, long count, Object value) {
        try {
            Long remove = redisTemplate.opsForList().remove(key, count, value);
            return remove;
        } catch (Exception e) {
            e.printStackTrace();
            return 0;
        }
    }
}
```



##### 测试

```java
@SpringBootTest(classes = RedisSampleApplication.class)
public class redissample {

    @Autowired
    private RedisUtils redisUtils;

    @Test
    void doTest() {
        User user = new User("小明", "18");
        redisUtils.set("aa",user.toString());
        redisUtils.set("master", "from client 7002");
        System.out.println(redisUtils.get("aa"));
    }

    @Test
    void doTest2() {
//        User user = new User("小明", "18");
//        redisUtils.get("aa",user.toString());
        System.out.println(redisUtils.get("aa"));
        System.out.println(redisUtils.get("bb"));
    }
}
```



### 待处理

```
https://www.cnblogs.com/javazhiyin/p/13839357.html
```



### 异常记录

现象：项目本地跑，长时间不连接，突然用web连接上去，怀疑是连接长时间不用，被mongodb断开了

```
Closed connection [connectionId{localValue:4, serverValue:182}] to www.gtom.top:27017 because there was a socket exception raised by this connection.
```



### 常见题目

#### 什么是 Redis

`Reids 是一种基于内存的可持久化的非关系型数据库，使用键值对方式存储数据，提供了多种数据类型，支持事务、持久化、主从复制、故障转移以及集群`



#### Redis 的优缺点

**优点**

- 读写性能高，官方给出数据：读11w次/s，写8.1w次/s
- 支持数据持久化，有 rdb 和 aof 两种持久化方式
- 支持事务，Redis 中一条命令的完成是原子性的，同时还支持使用事务实现多条命令的集合的原子性，但仅保证编译期间的原子性(即代码出错)，而不保证运行期间的原子性
- 多种数据结构，支持日常使用的同时，还支持地理位置存储、基数存储和位图存储
- 高可用：支持主从复制、故障转移、集群

**缺点**

- 高可用部署复杂，实现故障转移还需要部署哨兵节点集群
- 难以实现在线扩容，因此在上线前就需要确保有足够的空间



#### Redis 为什么是单线程的

因为 Redis 是基于内存操作的，数据的读写都在内存中，因而 CPU 不是 Redis 的瓶颈，内存和网络带宽才是 Redis 的瓶颈，就无需使用多线程来处理。同时，使用单线程还能避免线程上下文切换的开销，对于内存系统来说，没有上下文切换的消耗，就是效率最高的情况



#### 为什么 Redis 这么快

- 完全基于内存，请求处理效率十分快速
- 数据结构简单，对数据操作也简单
- 采用单线程，避免了不必要的上下文切换和竞争，不存在线程切换的消耗，也不用考虑锁的问题，不存在加锁释放锁的问题
- 使用多路复用IO



#### Redis 的数据类型

Redis 支持五种基本数据类型，三种特殊类型，分别为：

- 五种基本数据类型

  - String 做简单的数据存储，对 value 为整形或浮点数的内容进行自增或自减操作，可以用于存储用户关注数等。如

    ```markdown
    set uid:7830298:follow 0
    # 关注自增
    incr uid:7830298:follow
    ```

  - List 双向列表，可以存储列表形式的数据结构，如粉丝列表，文章评论列表等，以及实现缓冲队列

  - Hash 无序的键值对集合，可以用于存储用户数据

  - Set 无序的不重复集合，交集、差集等，可以共同朋友等

  - ZSet 有序Set，自动排序，实现排行版等

- 三种特殊类型

  - Geospatial 地理位置，支持获取两个地理位置之间的间距，范围内的地理位置，可以用于实现`附近的人、地图导航`等
  - Hyperloglog 基数，支持获取差集、并集以及合集，可以用于`推荐的朋友、共同朋友`等
  - Bitmaps 位图，支持以更小的内存消耗存放数据，如考勤等



#### Redis 的应用场景

- 计数器、关注、点赞
  - 可以对 String 进行自增自减运算，从而实现计数器功能。由于 Redis 的读写性能非常高，很适合用于频繁读写。如用户的关注，点赞数，然后每隔一段时间在存储到数据库中
- 缓存
  - 将热点数据放到内存中，降低数据库的访问压力。需要设置内存的最大使用量以及淘汰策略以保证缓存的命中率。
- 全局会话缓存
  - 使用 Redis 存储用户的 Session，用户可以访问项目中的任意服务器，服务器可以从 Redis 读取用户的 Session 状态来判断是否允许访问。同时可以设置 Redis 中存储的 Session 过期时间，实现超时管理
- 消息队列
  - Redis 支持发布/订阅功能，哨兵机制就是基于发布/订阅功能实现的
- 分布式锁
  - Redis 支持使用 set 实现分布式锁，实现多线程访问同一个变量的线程安全
  - https://www.cnblogs.com/agilestyle/p/13200032.html
  - https://www.cnblogs.com/houchen/p/13801103.html
- 共同好友、好友推荐
  - 使用 Set 或是 Hyperloglog 的交集实现共同好友，或是好友推荐
- 排行版
  - ZSet 维护了一个有序的不重复集合，可以很方便地实现排行版功能



#### Redis 的持久化机制

持久化就是将内存的数据写入到硬盘中。Redis 默认使用 rdb 方式实现持久化，同时也可以手动开启 aof 来实现持久化，两者可以同时运行

##### RDB 机制

`Redis Databases 是 Redis 默认的持久化机制，通过制定一定的触发规则，当触发时才会将数据写入到硬盘中，形成 rdb 文件。`

**修改方式：**

可以在 redis.conf 中配置 save 来修改，如 xx秒至少写了xx次，就触发一次 rdb 写入



**触发的情况有：**

- save 的规则满足的情况下，会自动触发 rdb 规则
- 退出 redis 时也会产生 rdb 文件
- 执行 flushall 命令，也会触发



**流程：**

- Redis 父进程会先判断当前是否有其他子进程在执行，若没有则 fork 一个子进程，此时父进程是阻塞的
- 父进程 fork 子进程后，会接收到 "background saving started" 信息，则父进程不在阻塞
- 子进程创建临时 rdb 文件，并根据父进程的内存快照生成临时快照文件，然后对原有的文件进行原子替换
- 替换完成后，子进程会发送信号通知父进程，父进程更新统计信息



##### AOF 机制

`Append Only File 即文件追加方式，是以日志的形式记录每个写操作，将 Redis 执行过得写操作(插入、更新、删除)写入到文件中。与 rdb 不同，aof 文件只能添加，而不能替换文件。支持每秒写入或是每次都写入`



**修改方式**

修改 redis.conf 文件中的 appendonly 为 yes 来开启 aof，通过 appendfsync 来修改写入策略，always 为每次写操作都会同步，everysec 为每秒同步



##### rdb 和 aof 对比

- **适用场景：**rdb 适用于大量数据恢复的情况，由于每次 rdb 文件的生成和复制是子进程处理的，不会阻塞主线程处理业务。而 aof 中增加日志则是主线程处理的
- **数据完整性：**rdb 可能丢失更多数据，而 aof 丢失的数据更少
- **性能消耗：**aof 会触发频繁的io操作，性能较低
- **日志文件大小：**rdb 是存储内存数据，而 aof 文件存储的是操作命令，无疑 aof 文件占用的内存空间会更大



#### Redis 扩容

Redis 中数据存储是通过 dict 字典实现的，内部使用了哈希表存储数据，因此 Redis 扩容其实就是对字典中的哈希表进行 rehash 操作。

- 若 Redis 仅使用主从模式，则是使用`一致性哈希`的方式支持扩容
- Redis 集群模式改为分配 hash 槽(一共16384个槽)的方式进行扩容，增加和删除节点都需要重新分配 hash 槽



#### Redis 如何删除过期键

Redis 有淘汰策略和过期策略。淘汰策略可以定义当内存满时，如何回收内存，默认是`只对设置了过期时间的数据进行LRU`，而过期策略则是当一个键过期后什么时候被回收。默认使用`惰性回收+定时回收结合`的方式



#### Redis 实现分布式锁



#### Redis 的 String 类型存储的 value 最大值是多少？

512 M



#### Redis 的 Hash 结构的底层结构

https://www.cnblogs.com/ysocean/p/9080942.html#_label4



#### Redis 的 ZSet 结构的底层结构

https://blog.csdn.net/weichi7549/article/details/107335133



#### Redis 的淘汰策略

有什么淘汰策略，内存不够了怎么办。all-keys 淘汰策略



#### 如何自己实现 LRU 算法

https://blog.csdn.net/qq_38297377/article/details/88325206



#### 持久化策略

rdb 和 aof



#### Redis 支持事务么

运行时异常会继续执行，编译时异常(如命令错误)就会整体回滚



#### Redis 实现接口幂等性

https://zhuanlan.zhihu.com/p/102776237?utm_source=wechat_session

https://www.weixiuzhan.cn/news/show-31267.html













## 其他

https://zhuanlan.zhihu.com/p/137368446



## #### 我是底 ####



































